{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGrader for Jupyter/Python scripts submitted to Blackboard\n",
    "This is a program that allows you to quickly grade student code by making simple annotations to your solution files. It compares the outputs of your solutions to the outputs of the student solutions when given the same set of parameters.\n",
    "\n",
    "## Installation\n",
    "Before proceeding make sure you install the required packages using REQUIREMENTS.txt\n",
    "\n",
    "## Limitations\n",
    "* Requires a Unix machine to run auto-grading as the timeout functionality uses Unix Signals\n",
    "* You cannot use underscores in the titles of the Blackboard assignments\n",
    "* Cannot grade class initializers (\\_\\_init\\_\\_ functions)\n",
    "* Students cannot use global variables as code outside functions and classes is deleted by code_parser.py\n",
    "* Can only grade top-level functions and top-level classes, no classes within classes or anything like that\n",
    "* Does not currently support comparing of print() statements (It could be done, but it's just easier if you make the students return a string)\n",
    "* All problems are graded from 0 to 1 and weighted equally in the final score (not counting extra credit). If you want to change the scalings you can do so with the output Excel spreadsheet.\n",
    "\n",
    "## TODO\n",
    "* Allow each problem to pass timeout_s instead of defining a single one. Currently, it is a constant in student_code.py\n",
    "\n",
    "## Before Starting\n",
    "* Please add \\_\\_str()\\_\\_ and \\_\\_repr()\\_\\_ functions to your classes so the student gets more detailed feedback on their failed test cases\n",
    "\n",
    "# How It Works (Design and Implementation)\n",
    "The basic principle is that we want to compare the output of our solution to the output of the student's solution when given the same set of parameters. If they differ, or an exception occurs, or the student code takes too long, we count it as a failed test case and give as much detailed feedback to the student as possible about why their code didn't pass the test case. We tell the AutoGrader how to generate sets of parameters through function annotations, which allows us to quickly grade any assignment.\n",
    "\n",
    "The general pipeline is as follows:\n",
    "1. Prepare grader by parsing command-line arguments and creating directories to be used later\n",
    "2. Override some libraries we don't want to run while grading such as matplotlib's show() or Colab's file.upload()\n",
    "3. Import the solution file functions and classes while parsing the annotations\n",
    "4. Convert jupyter .ipynb notebooks to .py files\n",
    "5. Grade all student code files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Run\n",
    "Let's say we are grading an exercise named \"Example Exercise 1\" on Blackboard for which we have a solution file called **example_exercise1_solution.py**. You would then follow these steps.\n",
    "\n",
    "1. Download all the submissions as a single zip file from Blackboard following these instructions:\n",
    "> * Go to the Grade Center\n",
    "> * Click the arrow next to the assignment column\n",
    "> * Select **File Assignment Download**\n",
    "> * Select all students and all attempts\n",
    "> * Click **Download** and then download the file\n",
    "\n",
    "2. Rename the zip file to **example_exercise1.zip** and move it to the \"**\\_data\\_**\" folder. **YOU DON'T NEED TO EXTRACT** as the grader will do it automatically and create a folder named **example_exercise1** with all the student submissions inside. If you do extract them, pass the directory as command-line argument **--student_dir**\n",
    "\n",
    "3. Move the solution file to the \"**\\_solution\\_**\" folder. You can place it anywhere in your computer, just make sure to pass the correct path to command-line argument **solution_file**\n",
    "\n",
    "4. Your directory structure should then look like this:\n",
    "```\n",
    "github-clone/_data_/example_exercise1.zip <---- This is the blackboard file\n",
    "github-clone/_solutions_/example_exercise1_solution.py <----- Annotated solution\n",
    "```\n",
    "\n",
    "5. Annotate the solution file (details in next section)\n",
    "\n",
    "6. Run **run_grader.py** with the correct command-line parameters\n",
    "\n",
    "The command-line arguments of **run_grader.py** are:\n",
    "* --solution_file (path to solution file)\n",
    "* --student_dir (path to folder where zip file is)\n",
    "* --max_grade (float)\n",
    "* --multiprocessing (number of CPU cores to use, optional)\n",
    "* --students (utep usernames of specific students to grade, separated by spaces, optional)\n",
    "\n",
    "And they also have short-hand names being:\n",
    "* -sol (...)\n",
    "* -sd (...)\n",
    "* -mg (...)\n",
    "* -mp (...)\n",
    "* -s (...)\n",
    "\n",
    "For example, grade the example assignment with a max grade of 100, grading 15 students in parallel with 15 cores\n",
    "> python run_grader.py -sol \\_solutions\\_/example\\_exercise1\\_solution.py -sd \\_data\\_/example\\_exercise1 -mg 100 -mp 15\n",
    "\n",
    "If **student_dir** has the same name as the solution (minus the \"_solution\" part) and is located in the \"**\\_data\\_**\" folder as in this example you don't need to pass it as an argument. The program will find it automatically. Thus, we can do\n",
    "> python run_grader.py -sol \\_solutions\\_/example\\_exercise1\\_solution.py -mg 100 -mp 15\n",
    "\n",
    "To grade individual students \"jperez\", \"ofuentes\", \"aarnal\" we can do\n",
    "> python run_grader.py -sol \\_solutions\\_/example\\_exercise1\\_solution.py -mg 100 -mp 15 **-s jperez ofuentes aarnal**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How To Annotate (Top-Level Functions)\n",
    "Let's say the solution function is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_has_k(L, k):\n",
    "    return k in L"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to generate random lists **L** and random numbers **k**. We can create a function that tells the grader how to generate each parameter as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate_list():\n",
    "    return list(np.random.random_integers(0, 100), 100)\n",
    "\n",
    "def generate_k():\n",
    "    return np.random.randint(-100, 200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we annotate the function with **@grader.generate_test_case(...)** and pass the functions with named parameters matching those in the graded function.\n",
    "\n",
    "We can also annotate functions with **@grader.no_test_cases()** when we want a function to not be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import grader\n",
    "\n",
    "@grader.generate_test_case(L = generate_list, k = generate_k)\n",
    "def list_has_k(L, k):\n",
    "    return k in L\n",
    "\n",
    "@grader.no_test_cases()\n",
    "def skip_grading():\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that @grader.generate_test_case() takes a **\\_\\_trials\\_\\_** parameter that defines how many test cases will be generated and which defaults to 2500 but can be easily changed.\n",
    "\n",
    "Also note that since Python supports lambda functions we can do something like this when the parameter generation functions are short:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import grader\n",
    "\n",
    "@grader.generate_test_case(__trials__ = 500,\n",
    "                           L = lambda: list(np.random.random_integers(0, 100), 100), \n",
    "                           k = lambda: np.random.randint(-100, 200))\n",
    "def list_has_k(L, k):\n",
    "    return k in L\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the AutoGrader compares the student output and the solution output to see if the student got the right answer. \n",
    "> You can check the details of this in the file **grader.py** through the function **compare_outputs()**.\n",
    "\n",
    "If you wanted more control, you can use a custom comparer annotation that gives you both the solution (sol) and student (stu) outputs and the parameters.\n",
    "\n",
    "In the following function we convert the outputs to sets to check if they are the same as they return lists in the original problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_function(solution_output, student_output, solution_parameters, student_parameters):\n",
    "    return set(solution_output) == set(student_output)\n",
    "\n",
    "#@grader.generate_test_case(S = ..., words = ...)\n",
    "@grader.generate_custom_comparer(compare_function)\n",
    "def words_in_largest_set(S, words):\n",
    "    ls = [len(s) for s in S]\n",
    "    ls = S[np.argmax(ls)]\n",
    "    wls = [words[i] for i in ls]\n",
    "    return wls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using lambda notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@grader.generate_test_case(S = ..., words = ...)\n",
    "@grader.generate_custom_comparer(lambda sol_out, stu_out, sol_params, stu_params: set(sol_out) == set(stu_out))\n",
    "def words_in_largest_set(S, words):\n",
    "    ls = [len(s) for s in S]\n",
    "    ls = S[np.argmax(ls)]\n",
    "    wls = [words[i] for i in ls]\n",
    "    return wls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Annotate (Class Functions)\n",
    "Let's say we have the following class and class functions which we want to grade.\n",
    "\n",
    "* The first function **lowercase_name()** takes no parameters and returns a string.\n",
    "\n",
    "* The second function **get_score_k(k)** takes 1 parameter and returns either None or an int.\n",
    "\n",
    "* The third function **compute_lab_average()** has no parameters, but doesn't return and instead sets a value inside the class.\n",
    "\n",
    "* The fourth function **print_info()** is a utility function and will not be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student:\n",
    "    def __init__(self, name = 'Jose', lab_scores = [100, 100, 100]):\n",
    "        self.name = name\n",
    "        self.lab_scores = lab_scores\n",
    "\n",
    "        self.average_lab = -1\n",
    "    \n",
    "    def lowercase_name(self):\n",
    "        return self.name.lower()\n",
    "    \n",
    "    def get_score_k(self, k):\n",
    "        if k < 0 or k >= len(self.lab_scores):\n",
    "            return None\n",
    "        return self.lab_scores[k]\n",
    "    \n",
    "    def compute_lab_average(self):\n",
    "        self.average_lab = sum(self.lab_scores) / len(self.lab_scores)\n",
    "\n",
    "    def print_info(self):\n",
    "        print('Name=', self.name, 'Scores=', self.lab_scores, 'Average=', self.average_lab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **import grader** and start by annotating the functions we don't want to be graded with **@grader.no_test_cases()**. Although **\\_\\_init\\_\\_** is ignored by default, we will add an annotation for clarity sake anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grader\n",
    "class Student:\n",
    "    @grader.no_test_cases()\n",
    "    def __init__(self, name = 'Jose', lab_scores = [100, 100, 100]):\n",
    "        self.name = name\n",
    "        self.lab_scores = lab_scores\n",
    "\n",
    "        self.average_lab = -1\n",
    "\n",
    "    @grader.no_test_cases()\n",
    "    def print_info(self):\n",
    "        print('Name=', self.name, 'Scores=', self.lab_scores, 'Average=', self.average_lab)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first function we need to tell the grader how to generate class instances. We do this by passing functions with named parameters matching those in the construction and using the **@grader.generate_class()** annotation.\n",
    "\n",
    "**DO NOT FORGET** to also add **@grader.generate_test_case()**. This function has no parameters so we can pass the annotation without any parameters as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grader\n",
    "\n",
    "\n",
    "def generate_name():\n",
    "    names = ['Jose', 'Olac', 'Diego']\n",
    "    return str(np.random.choice(names, 1)[0])\n",
    "\n",
    "\n",
    "def generate_lab_scores():\n",
    "    return np.random.random_integers(0, 100, 10)\n",
    "\n",
    "\n",
    "class Student:\n",
    "    @grader.no_test_cases()\n",
    "    def __init__(self, name='Jose', lab_scores=[100, 100, 100]):\n",
    "        self.name = name\n",
    "        self.lab_scores = lab_scores\n",
    "\n",
    "        self.average_lab = -1\n",
    "\n",
    "    @grader.generate_class(__trials_per_instance__ = 5,\n",
    "                           name = generate_name,\n",
    "                           lab_scores = generate_lab_scores)\n",
    "    @grader.generate_test_case()\n",
    "    def lowercase_name(self):\n",
    "        return self.name.lower()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **@grader.generate_class()** takes a **\\_\\_trials\\_per_instance\\_\\_** parameter which has **NO DEFAULT VALUE** and is required to be given. This parameter specifies how many test cases will be generated before a new class instance should be created.\n",
    "\n",
    "We can also use lambda notation as before to shorten this to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grader\n",
    "\n",
    "\n",
    "class Student:\n",
    "    @grader.no_test_cases()\n",
    "    def __init__(self, name='Jose', lab_scores=[100, 100, 100]):\n",
    "        self.name = name\n",
    "        self.lab_scores = lab_scores\n",
    "\n",
    "        self.average_lab = -1\n",
    "\n",
    "    @grader.generate_class(__trials_per_instance__=5,\n",
    "                           name=lambda: str(np.random.choice(['Jose', 'Olac', 'Diego'], 1)[0]),\n",
    "                           lab_scores=lambda: np.random.random_integers(0, 100, 10))\n",
    "    @grader.generate_test_case()\n",
    "    def lowercase_name(self):\n",
    "        return self.name.lower()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what we know so far we can easily annotate the second problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grader\n",
    "\n",
    "class Student:\n",
    "    @grader.no_test_cases()\n",
    "    def __init__(self, name='Jose', lab_scores=[100, 100, 100]):\n",
    "        self.name = name\n",
    "        self.lab_scores = lab_scores\n",
    "\n",
    "        self.average_lab = -1\n",
    "\n",
    "    @grader.generate_class(__trials_per_instance__=5,\n",
    "                           name=lambda: str(np.random.choice(['Jose', 'Olac', 'Diego'], 1)[0]),\n",
    "                           lab_scores=lambda: np.random.random_integers(0, 100, 10))\n",
    "    @grader.generate_test_case(k = lambda: np.random.randint(-5, 100))\n",
    "    def get_score_k(self, k):\n",
    "        if k < 0 or k >= len(self.lab_scores):\n",
    "            return None\n",
    "        return self.lab_scores[k]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third problem, we will use what we know so far and a custom comparer to compare the class variable \"self.average_lab\" as the function does not return anything but instead updates an internal class variable.\n",
    "\n",
    "To help us we will use **grader.compare_outputs()** which returns True when the two given parameters are equal to each other and is the utility function used internally for grading all other problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grader\n",
    "\n",
    "def compare_function(solution_instance, student_instance, solution_output, student_output, solution_parameters, student_parameters):\n",
    "    return grader.compare_outputs(solution_instance.average_lab, student_instance.average_lab)\n",
    "\n",
    "class Student:\n",
    "    @grader.no_test_cases()\n",
    "    def __init__(self, name='Jose', lab_scores=[100, 100, 100]):\n",
    "        self.name = name\n",
    "        self.lab_scores = lab_scores\n",
    "\n",
    "        self.average_lab = -1\n",
    "\n",
    "    @grader.generate_class(__trials_per_instance__=5,\n",
    "                           name=lambda: str(np.random.choice(['Jose', 'Olac', 'Diego'], 1)[0]),\n",
    "                           lab_scores=lambda: np.random.random_integers(0, 100, 10))\n",
    "    @grader.generate_test_case()\n",
    "    @grader.generate_custom_comparer(compare_function)\n",
    "    def compute_lab_average(self):\n",
    "        self.average_lab = sum(self.lab_scores) / len(self.lab_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using lambda notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grader\n",
    "\n",
    "class Student:\n",
    "    @grader.no_test_cases()\n",
    "    def __init__(self, name='Jose', lab_scores=[100, 100, 100]):\n",
    "        self.name = name\n",
    "        self.lab_scores = lab_scores\n",
    "\n",
    "        self.average_lab = -1\n",
    "\n",
    "    @grader.generate_class(__trials_per_instance__=5,\n",
    "                           name=lambda: str(np.random.choice(['Jose', 'Olac', 'Diego'], 1)[0]),\n",
    "                           lab_scores=lambda: np.random.random_integers(0, 100, 10))\n",
    "    @grader.generate_test_case()\n",
    "    @grader.generate_custom_comparer(lambda sol_instnc, stu_instnc, sol_output, stu_output, sol_params, stu_params: grader.compare_outputs(sol_instnc.average_lab, stu_instnc.average_lab))\n",
    "    def compute_lab_average(self):\n",
    "        self.average_lab = sum(self.lab_scores) / len(self.lab_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.11 (default, Jul 27 2021, 14:32:16) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2c224a93c8766fe4ea08c7f17fdb2ade40366f352d348d14df97b11df4144bfc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
